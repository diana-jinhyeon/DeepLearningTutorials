{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#tensorboard --logdir=path/to/log-directory\n",
    "#TensorBoard operates by reading TensorFlow events files,\n",
    "#  which contain summary data that you can generate when running TensorFlow.\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean/' + name, mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev/' + name, stddev)\n",
    "    tf.summary.scalar('max/' + name, tf.reduce_max(var))\n",
    "    tf.summary.scalar('min/' + name, tf.reduce_min(var))\n",
    "    tf.summary.histogram(name, var)\n",
    "    \n",
    "# Create Session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Input data\n",
    "with tf.name_scope('input'):\n",
    "  x = tf.placeholder(tf.float32, shape=[None, 784], name=\"placeholder_MNIST\") #x-input\n",
    "  y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"placeholder_TrueLabel\") #y-input\n",
    "\n",
    "\n",
    "# Weight Initialization\n",
    "def weight_variable(shape, name):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial, name)\n",
    "\n",
    "# Convolution and Pooling\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x, name):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME', name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape Input\n",
    "with tf.name_scope('input_reshape'):\n",
    "  x_image = tf.reshape(x, [-1,28,28,1], name=\"x_image_reshape\") # ?, width, height, Dim of Color   # 1 dim -> 4d tensor\n",
    "  tf.summary.image('input', x_image, 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Conv_Layer1\"):\n",
    "  # First Convolutional Layer\n",
    "  W_conv1 = weight_variable([5, 5, 1, 32], name=\"W_conv1\")  # 5x5 filter, Num of Input channel == 1, 32 features\n",
    "  b_conv1 = bias_variable([32], name=\"b_conv1\")\n",
    "  variable_summaries(W_conv1, 'W_conv1')\n",
    "  variable_summaries(b_conv1, 'b_conv1')\n",
    "\n",
    "  h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, name=\"h_conv1\")\n",
    "  variable_summaries(h_conv1, 'h_conv1')\n",
    "\n",
    "with tf.name_scope(\"Pooling_Layer1\"):\n",
    "  h_pool1 = max_pool_2x2(h_conv1, name=\"h_pool1\")\n",
    "  variable_summaries(h_pool1, 'h_pool1')\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Conv_Layer2\"):\n",
    "  W_conv2 = weight_variable([5, 5, 32, 64], name=\"W_conv2\")\n",
    "  b_conv2 = bias_variable([64], name=\"b_conv2\")\n",
    "  variable_summaries(W_conv2, 'W_conv2')\n",
    "  variable_summaries(b_conv2, 'b_conv2')\n",
    "\n",
    "  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, name=\"h_conv2\")\n",
    "  variable_summaries(h_conv2, 'h_conv2')\n",
    "\n",
    "with tf.name_scope(\"Pooling_Layer2\"):\n",
    "  h_pool2 = max_pool_2x2(h_conv2, name=\"h_pool2\")\n",
    "  variable_summaries(h_pool2, 'h_pool2')\n",
    "\n",
    "\n",
    "\n",
    "# Fully Connected Layer\n",
    "with tf.name_scope(\"FC_Layer1\"):\n",
    "  W_fc1 = weight_variable([7 * 7 * 64, 1024], name=\"W_fc1\") # Now that the image size has been reduced to 7x7 (Pooiling 2번!)\n",
    "  b_fc1 = bias_variable([1024], name=\"b_fc1\")\n",
    "  variable_summaries(W_fc1, 'W_fc1')\n",
    "  variable_summaries(b_fc1, 'b_fc1')\n",
    "  h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64], name=\"h_pool2_flat\")\n",
    "  with tf.name_scope('Wx_plus_b_with_ReLU'):\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name=\"h_fc1\")\n",
    "\n",
    "\n",
    "\n",
    "# Dropout\n",
    "with tf.name_scope('dropout'):\n",
    "  keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name=\"h_fc1_drop\")\n",
    "  variable_summaries(keep_prob, 'dropout_keep_probability')\n",
    "  variable_summaries(h_fc1_drop, 'h_fc1_drop')\n",
    "\n",
    "\n",
    "# Readout Layer (FC 2)\n",
    "with tf.name_scope(\"FC_Layer2\"):\n",
    "  W_fc2 = weight_variable([1024, 10], name=\"W_fc2\")\n",
    "  b_fc2 = bias_variable([10], name=\"b_fc2\")\n",
    "  variable_summaries(W_fc2, 'W_fc2')\n",
    "  variable_summaries(b_fc2, 'b_fc2')\n",
    "  with tf.name_scope('Wx_plus_b'):\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    tf.summary.histogram('y_conv', y_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and Evaluate the Model\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_), name=\"cross_entropy_ops\")\n",
    "  tf.summary.scalar('cross entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "  train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "  with tf.name_scope('correct_prediction'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1), name=\"correct_prediction\") # argmax(input,  dimension of the input Tensor to reduce across)\n",
    "    #tf.scalar_summary('correct_prediction', correct_prediction) #에러 발생\n",
    "  with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "# Variables are saved in binary files that, roughly, contain a map from variable names to tensor values.\n",
    "saver = tf.train.Saver() # 저장 대상이 모든 Variables 경우\n",
    "# Add ops to save and restore only 'v2' using the name \"my_v2\"\n",
    "#saver = tf.train.Saver({\"my_v2\": v2}) # 저장 대상이 일부 Variables 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('./train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter('./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.06\n",
      "Adding run metadata for 99\n",
      "step 100, training accuracy 0.88\n",
      "Adding run metadata for 199\n",
      "step 200, training accuracy 0.82\n",
      "Adding run metadata for 299\n",
      "step 300, training accuracy 0.88\n",
      "Adding run metadata for 399\n",
      "step 400, training accuracy 0.9\n",
      "Adding run metadata for 499\n",
      "step 500, training accuracy 0.94\n",
      "Adding run metadata for 599\n",
      "step 600, training accuracy 0.96\n",
      "Adding run metadata for 699\n",
      "step 700, training accuracy 0.94\n",
      "Adding run metadata for 799\n",
      "step 800, training accuracy 0.92\n",
      "Adding run metadata for 899\n",
      "step 900, training accuracy 0.98\n",
      "Adding run metadata for 999\n",
      "step 1000, training accuracy 0.94\n",
      "Adding run metadata for 1099\n",
      "step 1100, training accuracy 0.98\n",
      "Adding run metadata for 1199\n",
      "step 1200, training accuracy 0.98\n",
      "Adding run metadata for 1299\n",
      "step 1300, training accuracy 0.98\n",
      "Adding run metadata for 1399\n",
      "step 1400, training accuracy 0.98\n",
      "Adding run metadata for 1499\n",
      "step 1500, training accuracy 1\n",
      "Adding run metadata for 1599\n",
      "step 1600, training accuracy 0.96\n",
      "Adding run metadata for 1699\n",
      "step 1700, training accuracy 0.96\n",
      "Adding run metadata for 1799\n",
      "step 1800, training accuracy 0.98\n",
      "Adding run metadata for 1899\n",
      "step 1900, training accuracy 1\n",
      "Adding run metadata for 1999\n",
      "step 2000, training accuracy 0.98\n",
      "Adding run metadata for 2099\n",
      "step 2100, training accuracy 1\n",
      "Adding run metadata for 2199\n",
      "step 2200, training accuracy 0.96\n",
      "Adding run metadata for 2299\n",
      "step 2300, training accuracy 1\n",
      "Adding run metadata for 2399\n",
      "step 2400, training accuracy 0.98\n",
      "Adding run metadata for 2499\n",
      "step 2500, training accuracy 0.98\n",
      "Adding run metadata for 2599\n",
      "step 2600, training accuracy 1\n",
      "Adding run metadata for 2699\n",
      "step 2700, training accuracy 1\n",
      "Adding run metadata for 2799\n",
      "step 2800, training accuracy 0.96\n",
      "Adding run metadata for 2899\n",
      "step 2900, training accuracy 0.98\n",
      "Adding run metadata for 2999\n",
      "step 3000, training accuracy 0.98\n",
      "Adding run metadata for 3099\n",
      "step 3100, training accuracy 0.98\n",
      "Adding run metadata for 3199\n",
      "step 3200, training accuracy 1\n",
      "Adding run metadata for 3299\n",
      "step 3300, training accuracy 1\n",
      "Adding run metadata for 3399\n",
      "step 3400, training accuracy 0.98\n",
      "Adding run metadata for 3499\n",
      "step 3500, training accuracy 1\n",
      "Adding run metadata for 3599\n",
      "step 3600, training accuracy 0.98\n",
      "Adding run metadata for 3699\n",
      "step 3700, training accuracy 1\n",
      "Adding run metadata for 3799\n",
      "step 3800, training accuracy 0.98\n",
      "Adding run metadata for 3899\n",
      "step 3900, training accuracy 0.96\n",
      "Adding run metadata for 3999\n",
      "step 4000, training accuracy 0.98\n",
      "Adding run metadata for 4099\n",
      "step 4100, training accuracy 0.94\n",
      "Adding run metadata for 4199\n",
      "step 4200, training accuracy 0.98\n",
      "Adding run metadata for 4299\n",
      "step 4300, training accuracy 0.98\n",
      "Adding run metadata for 4399\n",
      "step 4400, training accuracy 0.98\n",
      "Adding run metadata for 4499\n",
      "step 4500, training accuracy 1\n",
      "Adding run metadata for 4599\n",
      "step 4600, training accuracy 0.98\n",
      "Adding run metadata for 4699\n",
      "step 4700, training accuracy 1\n",
      "Adding run metadata for 4799\n",
      "step 4800, training accuracy 0.98\n",
      "Adding run metadata for 4899\n",
      "step 4900, training accuracy 0.98\n",
      "Adding run metadata for 4999\n",
      "step 5000, training accuracy 1\n",
      "Adding run metadata for 5099\n",
      "step 5100, training accuracy 0.98\n",
      "Adding run metadata for 5199\n",
      "step 5200, training accuracy 1\n",
      "Adding run metadata for 5299\n",
      "step 5300, training accuracy 0.98\n",
      "Adding run metadata for 5399\n",
      "step 5400, training accuracy 1\n",
      "Adding run metadata for 5499\n",
      "step 5500, training accuracy 1\n",
      "Adding run metadata for 5599\n",
      "step 5600, training accuracy 1\n",
      "Adding run metadata for 5699\n",
      "step 5700, training accuracy 1\n",
      "Adding run metadata for 5799\n",
      "step 5800, training accuracy 1\n",
      "Adding run metadata for 5899\n",
      "step 5900, training accuracy 0.98\n",
      "Adding run metadata for 5999\n",
      "step 6000, training accuracy 1\n",
      "Adding run metadata for 6099\n",
      "step 6100, training accuracy 0.98\n",
      "Adding run metadata for 6199\n",
      "step 6200, training accuracy 1\n",
      "Adding run metadata for 6299\n",
      "step 6300, training accuracy 1\n",
      "Adding run metadata for 6399\n",
      "step 6400, training accuracy 0.98\n",
      "Adding run metadata for 6499\n",
      "step 6500, training accuracy 1\n",
      "Adding run metadata for 6599\n",
      "step 6600, training accuracy 1\n",
      "Adding run metadata for 6699\n",
      "step 6700, training accuracy 1\n",
      "Adding run metadata for 6799\n",
      "step 6800, training accuracy 0.98\n",
      "Adding run metadata for 6899\n",
      "step 6900, training accuracy 1\n",
      "Adding run metadata for 6999\n",
      "step 7000, training accuracy 0.98\n",
      "Adding run metadata for 7099\n",
      "step 7100, training accuracy 0.98\n",
      "Adding run metadata for 7199\n",
      "step 7200, training accuracy 0.98\n",
      "Adding run metadata for 7299\n",
      "step 7300, training accuracy 1\n",
      "Adding run metadata for 7399\n",
      "step 7400, training accuracy 1\n",
      "Adding run metadata for 7499\n",
      "step 7500, training accuracy 1\n",
      "Adding run metadata for 7599\n",
      "step 7600, training accuracy 1\n",
      "Adding run metadata for 7699\n",
      "step 7700, training accuracy 1\n",
      "Adding run metadata for 7799\n",
      "step 7800, training accuracy 1\n",
      "Adding run metadata for 7899\n",
      "step 7900, training accuracy 1\n",
      "Adding run metadata for 7999\n",
      "step 8000, training accuracy 1\n",
      "Adding run metadata for 8099\n",
      "step 8100, training accuracy 0.98\n",
      "Adding run metadata for 8199\n",
      "step 8200, training accuracy 0.98\n",
      "Adding run metadata for 8299\n",
      "step 8300, training accuracy 0.98\n",
      "Adding run metadata for 8399\n",
      "step 8400, training accuracy 1\n",
      "Adding run metadata for 8499\n",
      "step 8500, training accuracy 1\n",
      "Adding run metadata for 8599\n",
      "step 8600, training accuracy 0.98\n",
      "Adding run metadata for 8699\n",
      "step 8700, training accuracy 1\n",
      "Adding run metadata for 8799\n",
      "step 8800, training accuracy 1\n",
      "Adding run metadata for 8899\n",
      "step 8900, training accuracy 1\n",
      "Adding run metadata for 8999\n",
      "step 9000, training accuracy 1\n",
      "Adding run metadata for 9099\n",
      "step 9100, training accuracy 0.96\n",
      "Adding run metadata for 9199\n",
      "step 9200, training accuracy 1\n",
      "Adding run metadata for 9299\n",
      "step 9300, training accuracy 1\n",
      "Adding run metadata for 9399\n",
      "step 9400, training accuracy 1\n",
      "Adding run metadata for 9499\n",
      "step 9500, training accuracy 0.98\n",
      "Adding run metadata for 9599\n",
      "step 9600, training accuracy 1\n",
      "Adding run metadata for 9699\n",
      "step 9700, training accuracy 1\n",
      "Adding run metadata for 9799\n",
      "step 9800, training accuracy 0.98\n",
      "Adding run metadata for 9899\n",
      "step 9900, training accuracy 0.98\n",
      "Adding run metadata for 9999\n",
      "step 10000, training accuracy 0.98\n",
      "Adding run metadata for 10099\n",
      "step 10100, training accuracy 1\n",
      "Adding run metadata for 10199\n",
      "step 10200, training accuracy 1\n",
      "Adding run metadata for 10299\n",
      "step 10300, training accuracy 1\n",
      "Adding run metadata for 10399\n",
      "step 10400, training accuracy 0.96\n",
      "Adding run metadata for 10499\n",
      "step 10500, training accuracy 1\n",
      "Adding run metadata for 10599\n",
      "step 10600, training accuracy 1\n",
      "Adding run metadata for 10699\n",
      "step 10700, training accuracy 1\n",
      "Adding run metadata for 10799\n",
      "step 10800, training accuracy 1\n",
      "Adding run metadata for 10899\n",
      "step 10900, training accuracy 1\n",
      "Adding run metadata for 10999\n",
      "step 11000, training accuracy 1\n",
      "Adding run metadata for 11099\n",
      "step 11100, training accuracy 1\n",
      "Adding run metadata for 11199\n",
      "step 11200, training accuracy 1\n",
      "Adding run metadata for 11299\n",
      "step 11300, training accuracy 1\n",
      "Adding run metadata for 11399\n",
      "step 11400, training accuracy 1\n",
      "Adding run metadata for 11499\n",
      "step 11500, training accuracy 1\n",
      "Adding run metadata for 11599\n",
      "step 11600, training accuracy 1\n",
      "Adding run metadata for 11699\n",
      "step 11700, training accuracy 1\n",
      "Adding run metadata for 11799\n",
      "step 11800, training accuracy 1\n",
      "Adding run metadata for 11899\n",
      "step 11900, training accuracy 1\n",
      "Adding run metadata for 11999\n",
      "step 12000, training accuracy 1\n",
      "Adding run metadata for 12099\n",
      "step 12100, training accuracy 1\n",
      "Adding run metadata for 12199\n",
      "step 12200, training accuracy 1\n",
      "Adding run metadata for 12299\n",
      "step 12300, training accuracy 1\n",
      "Adding run metadata for 12399\n",
      "step 12400, training accuracy 1\n",
      "Adding run metadata for 12499\n",
      "step 12500, training accuracy 1\n",
      "Adding run metadata for 12599\n",
      "step 12600, training accuracy 1\n",
      "Adding run metadata for 12699\n",
      "step 12700, training accuracy 1\n",
      "Adding run metadata for 12799\n",
      "step 12800, training accuracy 1\n",
      "Adding run metadata for 12899\n",
      "step 12900, training accuracy 1\n",
      "Adding run metadata for 12999\n",
      "step 13000, training accuracy 0.98\n",
      "Adding run metadata for 13099\n",
      "step 13100, training accuracy 1\n",
      "Adding run metadata for 13199\n",
      "step 13200, training accuracy 1\n",
      "Adding run metadata for 13299\n",
      "step 13300, training accuracy 1\n",
      "Adding run metadata for 13399\n",
      "step 13400, training accuracy 1\n",
      "Adding run metadata for 13499\n",
      "step 13500, training accuracy 1\n",
      "Adding run metadata for 13599\n",
      "step 13600, training accuracy 1\n",
      "Adding run metadata for 13699\n",
      "step 13700, training accuracy 1\n",
      "Adding run metadata for 13799\n",
      "step 13800, training accuracy 1\n",
      "Adding run metadata for 13899\n",
      "step 13900, training accuracy 1\n",
      "Adding run metadata for 13999\n",
      "step 14000, training accuracy 1\n",
      "Adding run metadata for 14099\n",
      "step 14100, training accuracy 1\n",
      "Adding run metadata for 14199\n",
      "step 14200, training accuracy 0.98\n",
      "Adding run metadata for 14299\n",
      "step 14300, training accuracy 1\n",
      "Adding run metadata for 14399\n",
      "step 14400, training accuracy 1\n",
      "Adding run metadata for 14499\n",
      "step 14500, training accuracy 1\n",
      "Adding run metadata for 14599\n",
      "step 14600, training accuracy 1\n",
      "Adding run metadata for 14699\n",
      "step 14700, training accuracy 1\n",
      "Adding run metadata for 14799\n",
      "step 14800, training accuracy 1\n",
      "Adding run metadata for 14899\n",
      "step 14900, training accuracy 1\n",
      "Adding run metadata for 14999\n",
      "step 15000, training accuracy 1\n",
      "Adding run metadata for 15099\n",
      "step 15100, training accuracy 1\n",
      "Adding run metadata for 15199\n",
      "step 15200, training accuracy 1\n",
      "Adding run metadata for 15299\n",
      "step 15300, training accuracy 1\n",
      "Adding run metadata for 15399\n",
      "step 15400, training accuracy 1\n",
      "Adding run metadata for 15499\n",
      "step 15500, training accuracy 1\n",
      "Adding run metadata for 15599\n",
      "step 15600, training accuracy 1\n",
      "Adding run metadata for 15699\n",
      "step 15700, training accuracy 1\n",
      "Adding run metadata for 15799\n",
      "step 15800, training accuracy 1\n",
      "Adding run metadata for 15899\n",
      "step 15900, training accuracy 1\n",
      "Adding run metadata for 15999\n",
      "step 16000, training accuracy 1\n",
      "Adding run metadata for 16099\n",
      "step 16100, training accuracy 0.98\n",
      "Adding run metadata for 16199\n",
      "step 16200, training accuracy 1\n",
      "Adding run metadata for 16299\n",
      "step 16300, training accuracy 1\n",
      "Adding run metadata for 16399\n",
      "step 16400, training accuracy 1\n",
      "Adding run metadata for 16499\n",
      "step 16500, training accuracy 1\n",
      "Adding run metadata for 16599\n",
      "step 16600, training accuracy 1\n",
      "Adding run metadata for 16699\n",
      "step 16700, training accuracy 1\n",
      "Adding run metadata for 16799\n",
      "step 16800, training accuracy 1\n",
      "Adding run metadata for 16899\n",
      "step 16900, training accuracy 1\n",
      "Adding run metadata for 16999\n",
      "step 17000, training accuracy 1\n",
      "Adding run metadata for 17099\n",
      "step 17100, training accuracy 1\n",
      "Adding run metadata for 17199\n",
      "step 17200, training accuracy 1\n",
      "Adding run metadata for 17299\n",
      "step 17300, training accuracy 1\n",
      "Adding run metadata for 17399\n",
      "step 17400, training accuracy 1\n",
      "Adding run metadata for 17499\n",
      "step 17500, training accuracy 0.98\n",
      "Adding run metadata for 17599\n",
      "step 17600, training accuracy 1\n",
      "Adding run metadata for 17699\n",
      "step 17700, training accuracy 1\n",
      "Adding run metadata for 17799\n",
      "step 17800, training accuracy 1\n",
      "Adding run metadata for 17899\n",
      "step 17900, training accuracy 1\n",
      "Adding run metadata for 17999\n",
      "step 18000, training accuracy 0.98\n",
      "Adding run metadata for 18099\n",
      "step 18100, training accuracy 1\n",
      "Adding run metadata for 18199\n",
      "step 18200, training accuracy 1\n",
      "Adding run metadata for 18299\n",
      "step 18300, training accuracy 1\n",
      "Adding run metadata for 18399\n",
      "step 18400, training accuracy 1\n",
      "Adding run metadata for 18499\n",
      "step 18500, training accuracy 1\n",
      "Adding run metadata for 18599\n",
      "step 18600, training accuracy 1\n",
      "Adding run metadata for 18699\n",
      "step 18700, training accuracy 1\n",
      "Adding run metadata for 18799\n",
      "step 18800, training accuracy 1\n",
      "Adding run metadata for 18899\n",
      "step 18900, training accuracy 1\n",
      "Adding run metadata for 18999\n",
      "step 19000, training accuracy 1\n",
      "Adding run metadata for 19099\n",
      "step 19100, training accuracy 1\n",
      "Adding run metadata for 19199\n",
      "step 19200, training accuracy 1\n",
      "Adding run metadata for 19299\n",
      "step 19300, training accuracy 1\n",
      "Adding run metadata for 19399\n",
      "step 19400, training accuracy 1\n",
      "Adding run metadata for 19499\n",
      "step 19500, training accuracy 1\n",
      "Adding run metadata for 19599\n",
      "step 19600, training accuracy 1\n",
      "Adding run metadata for 19699\n",
      "step 19700, training accuracy 1\n",
      "Adding run metadata for 19799\n",
      "step 19800, training accuracy 1\n",
      "Adding run metadata for 19899\n",
      "step 19900, training accuracy 1\n",
      "Adding run metadata for 19999\n",
      "test accuracy 0.9922\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "\n",
    "  if i%100 == 0:\n",
    "    # Training Accuracy Code\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    summary, _ = sess.run([merged, train_step], feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    train_writer.add_summary(summary, i)\n",
    "\n",
    "    #Test Accuracy Code\n",
    "    #summary, acc = sess.run([merged, accuracy], feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n",
    "    #test_writer.add_summary(summary, i)\n",
    "    #print('step %d, Test Accuracy : %s' % (i, acc))\n",
    "\n",
    "  if i % 100 == 99:  # Record execution stats, Starting at 99 and This code will emit runtime statistics for every 100th step\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    summary, _ = sess.run([merged, train_step], feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5},\n",
    "                            options=run_options, run_metadata=run_metadata)\n",
    "    train_writer.add_run_metadata(run_metadata, 'step%d' % i)\n",
    "    train_writer.add_summary(summary, i)\n",
    "    print('Adding run metadata for', i)\n",
    "    \n",
    "  else:\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    summary, _ = sess.run([merged, train_step], feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    train_writer.add_summary(summary, i)\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ./save/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Save the variables to disk.\n",
    "# 폴더 미리 있어야함\n",
    "# 루프 안에서 주기적으로 저장할 경우는 아래와 같이\n",
    "# saver.save(sess, 'my-model', global_step=1000) ==> filename: 'my-model-1000'\n",
    "import os\n",
    "checkpoint_dir = \"./save\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "save_path = saver.save(sess, \"./save/model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "# Restore variables from disk.\n",
    "#saver.restore(sess, \"./save/model.ckpt\")\n",
    "#print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
