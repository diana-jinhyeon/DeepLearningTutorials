{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#tensorboard --logdir=path/to/log-directory\n",
    "#TensorBoard operates by reading TensorFlow events files,\n",
    "#  which contain summary data that you can generate when running TensorFlow.\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean/' + name, mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev/' + name, stddev)\n",
    "    tf.summary.scalar('max/' + name, tf.reduce_max(var))\n",
    "    tf.summary.scalar('min/' + name, tf.reduce_min(var))\n",
    "    tf.summary.histogram(name, var)\n",
    "\n",
    "# Create Session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Input data\n",
    "with tf.name_scope('input'):\n",
    "  x = tf.placeholder(tf.float32, shape=[None, 784], name=\"placeholder_MNIST\") #x-input\n",
    "  y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"placeholder_TrueLabel\") #y-input\n",
    "\n",
    "\n",
    "# Weight Initialization\n",
    "def weight_variable(shape, name):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial, name)\n",
    "\n",
    "# Convolution and Pooling\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x, name):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME', name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape Input\n",
    "with tf.name_scope('input_reshape'):\n",
    "  x_image = tf.reshape(x, [-1,28,28,1], name=\"x_image_reshape\") # ?, width, height, Dim of Color   # 1 dim -> 4d tensor\n",
    "  tf.summary.image('input', x_image, 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Conv_Layer1\"):\n",
    "  # First Convolutional Layer\n",
    "  W_conv1 = weight_variable([5, 5, 1, 32], name=\"W_conv1\")  # 5x5 filter, Num of Input channel == 1, 32 features\n",
    "  b_conv1 = bias_variable([32], name=\"b_conv1\")\n",
    "  variable_summaries(W_conv1, 'W_conv1')\n",
    "  variable_summaries(b_conv1, 'b_conv1')\n",
    "\n",
    "  h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, name=\"h_conv1\")\n",
    "  variable_summaries(h_conv1, 'h_conv1')\n",
    "\n",
    "with tf.name_scope(\"Pooling_Layer1\"):\n",
    "  h_pool1 = max_pool_2x2(h_conv1, name=\"h_pool1\")\n",
    "  variable_summaries(h_pool1, 'h_pool1')\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Conv_Layer2\"):\n",
    "  W_conv2 = weight_variable([5, 5, 32, 64], name=\"W_conv2\")\n",
    "  b_conv2 = bias_variable([64], name=\"b_conv2\")\n",
    "  variable_summaries(W_conv2, 'W_conv2')\n",
    "  variable_summaries(b_conv2, 'b_conv2')\n",
    "\n",
    "  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, name=\"h_conv2\")\n",
    "  variable_summaries(h_conv2, 'h_conv2')\n",
    "\n",
    "with tf.name_scope(\"Pooling_Layer2\"):\n",
    "  h_pool2 = max_pool_2x2(h_conv2, name=\"h_pool2\")\n",
    "  variable_summaries(h_pool2, 'h_pool2')\n",
    "\n",
    "\n",
    "\n",
    "# Fully Connected Layer\n",
    "with tf.name_scope(\"FC_Layer1\"):\n",
    "  W_fc1 = weight_variable([7 * 7 * 64, 1024], name=\"W_fc1\") # Now that the image size has been reduced to 7x7 (Pooiling 2번!)\n",
    "  b_fc1 = bias_variable([1024], name=\"b_fc1\")\n",
    "  variable_summaries(W_fc1, 'W_fc1')\n",
    "  variable_summaries(b_fc1, 'b_fc1')\n",
    "  h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64], name=\"h_pool2_flat\")\n",
    "  with tf.name_scope('Wx_plus_b_with_ReLU'):\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name=\"h_fc1\")\n",
    "\n",
    "\n",
    "\n",
    "# Dropout\n",
    "with tf.name_scope('dropout'):\n",
    "  keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name=\"h_fc1_drop\")\n",
    "  variable_summaries(keep_prob, 'dropout_keep_probability')\n",
    "  variable_summaries(h_fc1_drop, 'h_fc1_drop')\n",
    "\n",
    "\n",
    "# Readout Layer (FC 2)\n",
    "with tf.name_scope(\"FC_Layer2\"):\n",
    "  W_fc2 = weight_variable([1024, 10], name=\"W_fc2\")\n",
    "  b_fc2 = bias_variable([10], name=\"b_fc2\")\n",
    "  variable_summaries(W_fc2, 'W_fc2')\n",
    "  variable_summaries(b_fc2, 'b_fc2')\n",
    "  with tf.name_scope('Wx_plus_b'):\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    tf.summary.histogram('y_conv', y_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name cross entropy is illegal; using cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate the Model\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_), name=\"cross_entropy_ops\")\n",
    "  tf.summary.scalar('cross entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "  train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "  with tf.name_scope('correct_prediction'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1), name=\"correct_prediction\") # argmax(input,  dimension of the input Tensor to reduce across)\n",
    "    #tf.scalar_summary('correct_prediction', correct_prediction) #에러 발생\n",
    "  with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "  tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('./train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter('./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.1\n",
      "Adding run metadata for 99\n",
      "step 100, training accuracy 0.86\n",
      "Adding run metadata for 199\n",
      "step 200, training accuracy 0.84\n",
      "Adding run metadata for 299\n",
      "step 300, training accuracy 0.84\n",
      "Adding run metadata for 399\n",
      "step 400, training accuracy 0.92\n",
      "Adding run metadata for 499\n",
      "step 500, training accuracy 0.94\n",
      "Adding run metadata for 599\n",
      "step 600, training accuracy 1\n",
      "Adding run metadata for 699\n",
      "step 700, training accuracy 0.96\n",
      "Adding run metadata for 799\n",
      "test accuracy 0.9523\n"
     ]
    }
   ],
   "source": [
    "for i in range(800):\n",
    "\n",
    "  if i%100 == 0:\n",
    "    # Training Accuracy Code\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    summary, _ = sess.run([merged, train_step], feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    train_writer.add_summary(summary, i)\n",
    "\n",
    "    #Test Accuracy Code\n",
    "    #summary, acc = sess.run([merged, accuracy], feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n",
    "    #test_writer.add_summary(summary, i)\n",
    "    #print('step %d, Test Accuracy : %s' % (i, acc))\n",
    "\n",
    "  if i % 100 == 99:  # Record execution stats, Starting at 99 and This code will emit runtime statistics for every 100th step\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    summary, _ = sess.run([merged, train_step], feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5},\n",
    "                            options=run_options, run_metadata=run_metadata)\n",
    "    train_writer.add_run_metadata(run_metadata, 'step%d' % i)\n",
    "    train_writer.add_summary(summary, i)\n",
    "    print('Adding run metadata for', i)\n",
    "    \n",
    "  else:\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    summary, _ = sess.run([merged, train_step], feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    train_writer.add_summary(summary, i)\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
