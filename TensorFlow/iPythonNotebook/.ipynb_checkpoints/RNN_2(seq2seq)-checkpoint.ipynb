{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network(RNN) \n",
    "\n",
    "\n",
    "Recurrent neural network(RNN)은 Text, 음성, 시계열 데이터등 순차적인(Sequential) 데이터들에 대한 분석을 할때 주로 사용하는 알고리즘입니다. <br>\n",
    "본 문서에서는 지난시간에 배웠던 RNN을 활용해 '임의의 순서'를 갖는 문자열 데이터로부터<br> 'I LOVE YOU'라는 순서를 갖는 문자열을 출력하는 RNN을 학습시켜 보겠습니다.<br>\n",
    "\n",
    "본 예제에서는<br>\n",
    "Input: UOYEVOLI -> Output: ILOVEYOU <br>\n",
    "되도록 학습 시킬 것 입니다.<br><br>\n",
    "이렇게 input과 output이 모두 Sequence인 모델을 sequence-to-sequence(seq2seq) 또는 many-to-many model이라고 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint #데이터를 보기 좋게 출력해주는 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 처리에서는 보통 데이터를 다룰때 <br>\n",
    "1.단어사전) '텍스트데이터':'인덱스' <br>\n",
    "2.인덱스를 단어로변환) '인덱스':'텍스트데이터'<br>\n",
    "의 형태로 구축해서 사용합니다. <br>\n",
    "데이터를 처리할때 '텍스트데이터' 형태로 그대로 쓰기보다 '인덱스'를 사용해서 처리하는 것이 통상적이므로 여기서도 같은 방법을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L': 1, 'Y': 5, 'E': 4, 'O': 2, 'U': 6, 'I': 0, 'V': 3}\n",
      "{0: 'I', 1: 'L', 2: 'O', 3: 'V', 4: 'E', 5: 'Y', 6: 'U'}\n",
      "one_hot_dimension: 7\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "text_data = ['I','L','O','V','E','Y','O','U']\n",
    "\n",
    "\n",
    "# 단어사전 구축\n",
    "vocab = {}\n",
    "index = 0\n",
    "for text in text_data:\n",
    "    if text not in vocab:\n",
    "        vocab[text] = index\n",
    "        index = index + 1\n",
    "\n",
    "# 인덱스->단어 사전 구축\n",
    "index2Char = {}\n",
    "for text, index in vocab.items():\n",
    "    index2Char[index] = text\n",
    "\n",
    "print(vocab)\n",
    "print(index2Char)\n",
    "\n",
    "# one hot representation 차원 계산\n",
    "one_hot_dimension = len(vocab)\n",
    "print(\"one_hot_dimension:\", one_hot_dimension)\n",
    "\n",
    "# 넣어줄 데이터를 np.float32 타입으로 변환해줍니다. (지정하지 않아서 타입이 안맞을 경우 에러 발생)\n",
    "# 각 문자를 벡터로 나타내기 위해 one-hot representation 형태를 사용합니다.\n",
    "one_hot_embedding_matrix = np.array([[1, 0, 0, 0, 0, 0, 0],     # I\n",
    "                                       [0, 1, 0, 0, 0, 0, 0],  # L\n",
    "                                       [0, 0, 1, 0, 0, 0, 0],  # O\n",
    "                                       [0, 0, 0, 1, 0, 0, 0],  # V\n",
    "                                       [0, 0, 0, 0, 1, 0, 0],  # E\n",
    "                                       [0, 0, 0, 0, 0, 1, 0],  # Y\n",
    "                                       [0, 0, 0, 0, 0, 0, 1]], dtype=np.float32) # U\n",
    "\n",
    "print(one_hot_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_data [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "y_string ['I', 'L', 'O', 'V', 'E', 'Y', 'O', 'U']\n",
      "x_input_index [6, 2, 5, 4, 3, 2, 1, 0]\n",
      "x_input_string ['U', 'O', 'Y', 'E', 'V', 'O', 'L', 'I']\n",
      "x_input_one_hot\n",
      "[[[ 0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "y_data = [[0, 1, 2, 3, 4, 5, 2, 6]]    # Target은 ILOVEYOU 입니다.\n",
    "y_string = [index2Char[y] for y in y_data[0]]\n",
    "print(\"y_data\", y_data)\n",
    "print(\"y_string\", y_string)\n",
    "\n",
    "# I LOVE YOU를 거꾸로 뒤집은 \"UOY EVOL I\"을 임의의 문자열로 선택했습니다.\n",
    "x_input_char = ['U','O','Y','E','V','O','L','I']\n",
    "\n",
    "# 문자열을 index로 바꾸겠습니다.\n",
    "x_input_index = []\n",
    "\n",
    "for x in x_input_char:    \n",
    "    x_input_index.append(vocab[x])  #  (문자->index) 로 변환\n",
    "\n",
    "print(\"x_input_index\", x_input_index)    \n",
    "x_input_string = [index2Char[x] for x in x_input_index]\n",
    "print(\"x_input_string\", x_input_string)\n",
    "\n",
    "# index를 이용하여 one_hot_vector 모양으로 바꾸겠습니다.\n",
    "x_input_one_hot = []\n",
    "for x in x_input_index:\n",
    "    x_input_one_hot.append(one_hot_embedding_matrix[x])\n",
    "x_input_one_hot = np.array([x_input_one_hot]) #Tensorflow에서는 첫차원이 Batch size이므로 차원을 한칸 밀어줍니다.\n",
    "print(\"x_input_one_hot\")\n",
    "print(x_input_one_hot) # Batch, 문장내 문자수, 문자 차원수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "sequence_length = 8 #len(x_input_char)\n",
    "input_dimension = 7 #one_hot_dimension\n",
    "num_classes = 7 # 문자수\n",
    "hidden_size = 2 # RNN의 hidden layer 차원\n",
    "num_layers = 2 # Multi layer RNN의 layer 개수\n",
    "batch_size = 1 # 현재는 문장 1개로 하기 때문에 1\n",
    "learning_rate = 0.01 # Adam optimizer의 Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_input = tf.placeholder(tf.float32, [None, sequence_length, input_dimension]) #Padding까지 고려된 크기\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length]) # RNN의 output은 character에 대한 index (Class 혹은 Label)\n",
    "seq_length = tf.placeholder(tf.int32) #나중에 가변적인 값을 넣을 때 사용 ex) [['really','good'],['good', 'pad']] 의 경우 Seq_length는 [2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Bidirectional RNN\n",
    "지난 시간에 배운 Multi-Bidirectional RNN을 적용해보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Multi_Bi_RNN(x_input, sequence_length, hidden_size, num_layers, rnn_type='rnn'):\n",
    "    with tf.variable_scope('multi_bidirectional_rnn') as scope:\n",
    "        \n",
    "#         hidden_size = 2\n",
    "#         num_layers = 2\n",
    "        \n",
    "        cell_fw = None\n",
    "        cell_bw = None\n",
    "        if(rnn_type == 'rnn'):\n",
    "            cell_fw = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "            cell_bw = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "            \n",
    "        elif(rnn_type == 'lstm'):\n",
    "            cell_fw = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "            cell_bw = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)            \n",
    "            #state_is_tuple = True로 하면 Cell State와 Hidden State의 값을 tuple형태로 분리해서 보여줍니다.\n",
    "        cell_fw = tf.contrib.rnn.MultiRNNCell(cells=[cell_fw] * num_layers, state_is_tuple=True)\n",
    "        cell_bw = tf.contrib.rnn.MultiRNNCell(cells=[cell_bw] * num_layers, state_is_tuple=True)\n",
    "            \n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x_input, \n",
    "                                                          sequence_length=sequence_length,\n",
    "                                                          dtype=tf.float32)\n",
    "        return outputs, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs, states = Multi_Bi_RNN(X_input, seq_length, hidden_size, num_layers, rnn_type='lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer\n",
    "RNN에서 나온 값들을 Fully Connected Layer의 Input 값으로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_for_fc_0 = tf.reshape(outputs[0], [-1, hidden_size])\n",
    "# x_for_fc_1 = tf.reshape(outputs[1], [-1, hidden_size])\n",
    "# x_for_fc = tf.concat(x_for_fc_0, x_for_fc_1, axis = 1)\n",
    "\n",
    "x_for_fc = tf.reshape(outputs, [-1, hidden_size * 2]) # Bidirectional 이기 때문에 Output이 2개 나오기 때문에 hidden_size * 2\n",
    "\n",
    "outputs = tf.contrib.layers.fully_connected(\n",
    "    inputs=x_for_fc, num_outputs=num_classes, activation_fn=None, weights_initializer=tf.contrib.layers.xavier_initializer()) # Xavier Glorot and Yoshua Bengio (2010): Understanding the difficulty of training deep feedforward neural networks. \n",
    "#print(outputs) #Tensor(\"fully_connected/BiasAdd:0\", shape=(?, 7), dtype=float32)\n",
    "\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes]) #TODO Seq_length\n",
    "#print(outputs) #Tensor(\"Reshape_2:0\", shape=(1, 8, 7), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Loss 계산 및 Train op\n",
    "Time Step 마다 나오는 output과 거기에 대응되는 Target Y의 loss를 계산해줍니다.<br>\n",
    "Tensorflow에서는 이를 위한 API인 tf.contrib.seq2seq.sequence_loss를 제공해줍니다.<br>\n",
    "weights는 주로 padding이 있을때 masking 용도로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = tf.ones([batch_size, sequence_length])\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(outputs, axis=2) # 0: Batch_size, 1: Sequence_length, 2: Num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation\n",
    "정의한 Operation을 session에 넣고 training을 시작합니다.<br>\n",
    "모델이 예측하는 문자열의 순서를 확인합니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.94543 prediction:  [[2 2 2 2 6 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOOOUYOO\n",
      "1 loss: 1.93668 prediction:  [[2 2 2 2 2 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOOOOYOO\n",
      "2 loss: 1.92796 prediction:  [[2 2 2 2 2 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOOOOYOO\n",
      "3 loss: 1.91891 prediction:  [[2 2 2 3 2 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOOVOYOO\n",
      "4 loss: 1.90928 prediction:  [[2 2 2 3 2 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOOVOYOO\n",
      "5 loss: 1.89892 prediction:  [[2 2 3 3 5 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVYYOO\n",
      "6 loss: 1.88772 prediction:  [[2 2 3 3 5 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVYYOO\n",
      "7 loss: 1.8756 prediction:  [[2 2 3 3 5 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVYYOO\n",
      "8 loss: 1.86249 prediction:  [[2 2 3 3 5 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVYYOO\n",
      "9 loss: 1.84839 prediction:  [[2 2 3 3 5 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVYYOO\n",
      "10 loss: 1.83332 prediction:  [[2 2 3 3 5 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVYYOO\n",
      "11 loss: 1.81733 prediction:  [[2 2 3 3 5 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVYYOO\n",
      "12 loss: 1.80051 prediction:  [[2 2 3 3 5 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVYYOO\n",
      "13 loss: 1.78301 prediction:  [[2 2 3 3 5 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVYYOO\n",
      "14 loss: 1.76498 prediction:  [[2 3 3 3 5 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVYYOO\n",
      "15 loss: 1.74658 prediction:  [[2 3 3 3 5 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVYYYO\n",
      "16 loss: 1.72798 prediction:  [[2 3 3 3 5 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVYYYO\n",
      "17 loss: 1.70934 prediction:  [[2 3 3 3 5 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVYYYO\n",
      "18 loss: 1.69079 prediction:  [[2 3 3 3 5 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVYYYO\n",
      "19 loss: 1.67243 prediction:  [[2 3 3 3 5 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVYYYO\n",
      "20 loss: 1.65433 prediction:  [[2 3 3 3 5 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVYYYO\n",
      "21 loss: 1.63652 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "22 loss: 1.61899 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "23 loss: 1.60168 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "24 loss: 1.5845 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "25 loss: 1.56738 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "26 loss: 1.55028 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "27 loss: 1.53318 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "28 loss: 1.51609 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "29 loss: 1.49905 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "30 loss: 1.48212 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "31 loss: 1.46535 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "32 loss: 1.44881 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "33 loss: 1.43249 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "34 loss: 1.41637 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "35 loss: 1.40039 prediction:  [[2 3 3 3 4 5 5 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYYO\n",
      "36 loss: 1.3845 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "37 loss: 1.36871 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "38 loss: 1.35305 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "39 loss: 1.33762 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "40 loss: 1.32245 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "41 loss: 1.30759 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "42 loss: 1.293 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "43 loss: 1.27865 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "44 loss: 1.26454 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "45 loss: 1.2507 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "46 loss: 1.23715 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "47 loss: 1.22395 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "48 loss: 1.21108 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "49 loss: 1.19852 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "50 loss: 1.18622 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "51 loss: 1.17419 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "52 loss: 1.16244 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "53 loss: 1.15097 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "54 loss: 1.13979 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "55 loss: 1.12883 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "56 loss: 1.11809 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "57 loss: 1.10754 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "58 loss: 1.09719 prediction:  [[2 2 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVEYOO\n",
      "59 loss: 1.08703 prediction:  [[2 2 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVEYOO\n",
      "60 loss: 1.07707 prediction:  [[2 2 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVEYOO\n",
      "61 loss: 1.06727 prediction:  [[2 2 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVEYOO\n",
      "62 loss: 1.05763 prediction:  [[2 2 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVEYOO\n",
      "63 loss: 1.04814 prediction:  [[2 2 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OOVVEYOO\n",
      "64 loss: 1.03879 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "65 loss: 1.02957 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "66 loss: 1.02048 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "67 loss: 1.01147 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "68 loss: 1.00254 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "69 loss: 0.993667 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "70 loss: 0.984836 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "71 loss: 0.976038 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "72 loss: 0.967257 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "73 loss: 0.958477 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "74 loss: 0.949688 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "75 loss: 0.940881 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "76 loss: 0.93205 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "77 loss: 0.923184 prediction:  [[2 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  OVVVEYOO\n",
      "78 loss: 0.914278 prediction:  [[0 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVVVEYOO\n",
      "79 loss: 0.905325 prediction:  [[0 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVVVEYOO\n",
      "80 loss: 0.89632 prediction:  [[0 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVVVEYOO\n",
      "81 loss: 0.887256 prediction:  [[0 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVVVEYOO\n",
      "82 loss: 0.878132 prediction:  [[0 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVVVEYOO\n",
      "83 loss: 0.868947 prediction:  [[0 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVVVEYOO\n",
      "84 loss: 0.859702 prediction:  [[0 3 3 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVVVEYOO\n",
      "85 loss: 0.8504 prediction:  [[0 3 2 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOO\n",
      "86 loss: 0.841044 prediction:  [[0 3 2 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOO\n",
      "87 loss: 0.831642 prediction:  [[0 3 2 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOO\n",
      "88 loss: 0.822196 prediction:  [[0 3 2 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOO\n",
      "89 loss: 0.812715 prediction:  [[0 3 2 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOO\n",
      "90 loss: 0.803205 prediction:  [[0 3 2 3 4 5 2 2]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOO\n",
      "91 loss: 0.793671 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "92 loss: 0.784122 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "93 loss: 0.774563 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "94 loss: 0.765001 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "95 loss: 0.755443 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "96 loss: 0.745897 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "97 loss: 0.736368 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "98 loss: 0.726862 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "99 loss: 0.717386 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "100 loss: 0.707945 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "101 loss: 0.698544 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "102 loss: 0.689189 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "103 loss: 0.679884 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "104 loss: 0.670635 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "105 loss: 0.661445 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "106 loss: 0.652319 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "107 loss: 0.643261 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "108 loss: 0.634275 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "109 loss: 0.625363 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "110 loss: 0.616527 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "111 loss: 0.60777 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "112 loss: 0.599092 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "113 loss: 0.590492 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "114 loss: 0.581971 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "115 loss: 0.573528 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "116 loss: 0.56516 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "117 loss: 0.556867 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "118 loss: 0.548647 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "119 loss: 0.540498 prediction:  [[0 3 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  IVOVEYOU\n",
      "120 loss: 0.532418 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "121 loss: 0.524405 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "122 loss: 0.516458 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "123 loss: 0.508575 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "124 loss: 0.500757 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "125 loss: 0.493001 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "126 loss: 0.485308 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "127 loss: 0.477679 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "128 loss: 0.470113 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "129 loss: 0.462612 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "130 loss: 0.455177 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "131 loss: 0.44781 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "132 loss: 0.440513 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "133 loss: 0.433288 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "134 loss: 0.426138 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "135 loss: 0.419066 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "136 loss: 0.412074 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "137 loss: 0.405165 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "138 loss: 0.398342 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "139 loss: 0.391608 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "140 loss: 0.384964 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "141 loss: 0.378415 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "142 loss: 0.371962 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "143 loss: 0.365608 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "144 loss: 0.359354 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "145 loss: 0.353202 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "146 loss: 0.347154 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "147 loss: 0.341212 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "148 loss: 0.335377 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "149 loss: 0.329649 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "150 loss: 0.324029 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "151 loss: 0.318518 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "152 loss: 0.313115 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "153 loss: 0.307822 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "154 loss: 0.302636 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "155 loss: 0.297559 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "156 loss: 0.29259 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "157 loss: 0.287727 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "158 loss: 0.28297 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "159 loss: 0.278317 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "160 loss: 0.273767 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "161 loss: 0.269319 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "162 loss: 0.264972 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "163 loss: 0.260723 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "164 loss: 0.256571 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "165 loss: 0.252514 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "166 loss: 0.248551 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "167 loss: 0.244679 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "168 loss: 0.240896 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "169 loss: 0.237202 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "170 loss: 0.233593 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "171 loss: 0.230067 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "172 loss: 0.226624 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "173 loss: 0.22326 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "174 loss: 0.219975 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "175 loss: 0.216765 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "176 loss: 0.21363 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "177 loss: 0.210566 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "178 loss: 0.207573 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "179 loss: 0.204648 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "180 loss: 0.20179 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "181 loss: 0.198997 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "182 loss: 0.196267 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "183 loss: 0.193599 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "184 loss: 0.19099 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "185 loss: 0.188439 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "186 loss: 0.185945 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "187 loss: 0.183506 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "188 loss: 0.181121 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "189 loss: 0.178787 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "190 loss: 0.176504 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "191 loss: 0.174271 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "192 loss: 0.172085 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "193 loss: 0.169945 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "194 loss: 0.167851 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "195 loss: 0.165801 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "196 loss: 0.163793 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "197 loss: 0.161827 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "198 loss: 0.159902 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "199 loss: 0.158015 prediction:  [[0 1 2 3 4 5 2 6]] true Y:  [[0, 1, 2, 3, 4, 5, 2, 6]]\n",
      "RNN 모델의 예측 결과:  ILOVEYOU\n",
      "End:)\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X_input: x_input_one_hot, Y: y_data, seq_length:[8]}) # seq_length:[sequence_length] (Batch_size, Seq)\n",
    "        result = sess.run(prediction, feed_dict={X_input: x_input_one_hot, seq_length:[8]})\n",
    "        print(i, \"loss:\", l, \"prediction: \", result, \"true Y: \", y_data)\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [index2Char[c] for c in np.squeeze(result)]\n",
    "        print(\"RNN 모델의 예측 결과: \", ''.join(result_str))\n",
    "print(\"End:)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
