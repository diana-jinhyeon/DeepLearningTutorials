{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network(RNN) \n",
    "\n",
    "\n",
    "Recurrent neural network(RNN)은 Text, 음성, 시계열 데이터등 순차적인(Sequential) 데이터들에 대한 분석을 할때 주로 사용하는 알고리즘입니다. <br>\n",
    "본 문서에서는 RNN의 기본적인 내용부터 실습하겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint #데이터를 보기 좋게 출력해주는 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text 데이터를 어떻게 Neural Network에 넣을까?\n",
    "Text 데이터를 Neural Network에 넣는 방법은 벡터화(embedding)입니다.<br>\n",
    "Text 데이터를 벡터화 시키는 방법은 여러가지(One-hot, bag of words, tf-idf, word embedding)가 있습니다.<br>\n",
    "오늘은 그중 하나인 one-hot representation을 사용하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 각 문자를 벡터로 나타내기 위해 one-hot representation 형태로 나타냅니다.\n",
    "# I LOVE YOU 를 표현하기 위해 문자를 One-hot represnetation으로 나타내었습니다.\n",
    "I = [1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "L = [0, 1, 0, 0, 0, 0, 0]\n",
    "O = [0, 0, 1, 0, 0, 0, 0]\n",
    "V = [0, 0, 0, 1, 0, 0, 0]\n",
    "E = [0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "Y = [0, 0, 0, 0, 0, 1, 0]\n",
    "U = [0, 0, 0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 과 LSTM\n",
    "Tensorflow에서는 쉽게 RNN을 정의 할 수 있습니다. 아래에서는 편의상 함수의 형태로 정의하였습니다.<br>\n",
    "아래 선언된 RNN cell중 LSTM(Long Short-Term Memory)은 RNN의 Vanishing gradient 문제를 해결하고 이전 정보를 더욱 잘 반영하기 위해서 RNN의 Activation unit을 변경한 것입니다.\n",
    "![lstm](./assets/lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x_input, sequence_length, rnn_type='rnn'):\n",
    "    with tf.variable_scope('rnn') as scope:\n",
    "        \n",
    "        hidden_size = 2 # RNN의 Hidden Layer의 차원을 의미합니다.\n",
    "        \n",
    "        cell = None\n",
    "        if(rnn_type == 'rnn'):\n",
    "            cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "        elif(rnn_type == 'lstm'):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "            #state_is_tuple = True로 하면 State의 값을 tuple형태로 분리해서 보여줍니다.\n",
    "                                    \n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, x_input,\n",
    "                                            sequence_length=sequence_length, #[8], ILOVEYOU의 길이\n",
    "                                            dtype=tf.float32)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"outputs of RNN\")\n",
    "        print(sess.run(outputs))\n",
    "        print(\"states of RNN\")\n",
    "        pprint.pprint(sess.run(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow에서 데이터의 가장 첫번째 차원은 Batch_size !\n",
    "Tensorflow에서 데이터의 가장 첫번째 Dimension은 Batch에 해당하기 때문에 아래와 같이 사용할 데이터인<br>\n",
    "1차원에 속한 [I, L, O, V, E, Y, O, U]를 -> 2차원인 [[I, L, O, V, E, Y, O, U]]로 할당해 줍니다.<br>\n",
    "현재 차원 (batch_size, sequence_length, one_hot_representation_dimension) == (1, 8, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input.shape\n",
      "(1, 8, 7)\n",
      "x_input\n",
      "[[[ 1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.]]]\n"
     ]
    }
   ],
   "source": [
    "x_input = np.array([[I, L, O, V, E, Y, O, U]], dtype=np.float32) \n",
    "print(\"x_input.shape\")\n",
    "print(x_input.shape)\n",
    "print(\"x_input\")\n",
    "print(x_input)\n",
    "sequence_length = x_input.shape[1] #글자 길이\n",
    "sequence_length = [sequence_length] # 첫번째 차원이 Batch기 때문에 실수가 아닌 []로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선언한 RNN함수를 호출\n",
    "RNN함수를 호출합니다. <br>\n",
    "RNN의 각 Time step 별로 어떤 Output이 나오는지, RNN의 State값은 무엇인지를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of RNN\n",
      "[[[ 0.55137813  0.52583224]\n",
      "  [ 0.38262469  0.35530469]\n",
      "  [ 0.0363228  -0.25309616]\n",
      "  [-0.2310739  -0.17970082]\n",
      "  [-0.1220564   0.6164664 ]\n",
      "  [-0.14255311  0.62826061]\n",
      "  [ 0.50580305  0.04710453]\n",
      "  [-0.03026293  0.07864685]]]\n",
      "states of RNN\n",
      "array([[-0.03026293,  0.07864685]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "RNN(x_input, sequence_length, rnn_type='rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic_RNN\n",
    "텐서플로우에서는 Static_RNN과 Dynamic_RNN이 있습니다. <br>\n",
    "그러나 거의 대부분 Dynamic_RNN을 사용합니다. Dynamic_RNN은 Input data의 길이(sequence)를 고려하여 계산이 가능합니다.<br>\n",
    "아래 코드에서 길이가 8인 문자열에 대해서 길이 7만큼만 계산하면 결과가 어떻게 나오는지 확인해보시면 7이상인 경우 Output값이 0이 나옴을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of RNN\n",
      "[[[-0.19231324 -0.05708623]\n",
      "  [-0.04951089 -0.0964755 ]\n",
      "  [-0.14186911 -0.11884118]\n",
      "  [-0.21588489 -0.29764304]\n",
      "  [-0.19670735 -0.12379649]\n",
      "  [-0.20099474 -0.16918392]\n",
      "  [-0.30278811 -0.18010156]\n",
      "  [-0.29759625 -0.29654554]]\n",
      "\n",
      " [[-0.19231324 -0.05708623]\n",
      "  [-0.04951089 -0.0964755 ]\n",
      "  [-0.14186911 -0.11884118]\n",
      "  [-0.21588489 -0.29764304]\n",
      "  [-0.19670735 -0.12379649]\n",
      "  [-0.20099474 -0.16918392]\n",
      "  [-0.30278811 -0.18010156]\n",
      "  [ 0.          0.        ]]]\n",
      "states of RNN\n",
      "LSTMStateTuple(c=array([[-0.5437721 , -0.71608835],\n",
      "       [-0.63887435, -0.49504983]], dtype=float32), h=array([[-0.29759625, -0.29654554],\n",
      "       [-0.30278811, -0.18010156]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Sequence Length를 3으로 변경하면 Dynamic RNN의 특성상\n",
    "# Time Step 3번째 까지만 계산합니다.\n",
    "#tf.get_variable_scope().reuse_variables() # 같은 RNN을 재사용할때 필요한 코드\n",
    "x_input_2 = np.array([[I, L, O, V, E, Y, O, U],\n",
    "                      [I, L, O, V, E, Y, O, U]], dtype=np.float32) \n",
    "RNN(x_input_2, sequence_length=[8,7], rnn_type='lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"I LOVE YOU\" 문장의 길이가 총 8이므로 RNN의 Output도 각 Time Step별로 총 8개가 나옵니다. <br>RNN의 State는 같은 Neural Network를 반복적으로 쓰는 RNN의 특성 때문에 Time Step과 상관없이 하나의 값이 나오게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer RNN\n",
    "RNN을 여러개 쌓을 수도 있습니다. Tensorflow에서는 tf.contrib.rnn.MultiRNNCell이라는 API를 제공하며 아래와 같이 Multi Layer RNN을 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Multi_RNN(x_input, sequence_length, rnn_type='rnn'):\n",
    "    with tf.variable_scope('multi_rnn') as scope:\n",
    "        \n",
    "        hidden_size = 2\n",
    "        num_layers = 2\n",
    "        \n",
    "        cell = None\n",
    "        if(rnn_type == 'rnn'):\n",
    "            cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "        elif(rnn_type == 'lstm'):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "            #state_is_tuple = True로 하면 Cell State와 Hidden State의 값을 tuple형태로 분리해서 보여줍니다.\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells=[cell] * num_layers, state_is_tuple=True)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, x_input,\n",
    "                                            sequence_length=sequence_length,\n",
    "                                            dtype=tf.float32)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"outputs of Multi_RNN\")\n",
    "        print(sess.run(outputs))\n",
    "        print(\"states of Multi_RNN\")\n",
    "        pprint.pprint(sess.run(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Layer RNN의 경우 Output값은 가장 위에 층의 값을 사용하게 됩니다.\n",
    "State는 각 층당 1개씩 출력해주는걸 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of Multi_RNN\n",
      "[[[ 0.00077869  0.00561604]\n",
      "  [-0.02991019  0.01418542]\n",
      "  [-0.06328127  0.02125617]\n",
      "  [-0.06087604  0.02229038]\n",
      "  [-0.05649587  0.01666107]\n",
      "  [-0.07129844  0.020167  ]\n",
      "  [-0.09387885  0.02434255]\n",
      "  [-0.10773665  0.01925956]]]\n",
      "states of Multi_RNN\n",
      "(LSTMStateTuple(c=array([[ 0.19464058, -0.54733354]], dtype=float32), h=array([[ 0.07342119, -0.22338189]], dtype=float32)),\n",
      " LSTMStateTuple(c=array([[-0.21893743,  0.03750154]], dtype=float32), h=array([[-0.10773665,  0.01925956]], dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "Multi_RNN(x_input, sequence_length, rnn_type='lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bidirectional RNN\n",
    "빈칸 넣기 게임처럼 문맥을 고려해야 해결할 수 있을 때가 있습니다. <br>\n",
    "Bidirectional RNN은 이러한 경우를 고려한 뉴럴 네트워크입니다. <br>\n",
    "Tensorflow에서는 tf.nn.bidirectional_dynamic_rnn이라는 API를 통해 손쉽게 Bidirectional RNN을 구성할 수 있습니다. <br>\n",
    "\n",
    "### Tip\n",
    "Bidirectional RNN에 데이터가 들어가는 과정에 대해서 조금 더 자세히 말씀드리자면, Input 데이터의 순서를 '반대'로 바꿔서 집어 넣는다고 생각하시면 됩니다. 이러한 과정은 텐서플로우 내에서 자동으로 이루어 집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bi_RNN(x_input, sequence_length, rnn_type='rnn'):\n",
    "    with tf.variable_scope('bidirectional_rnn') as scope:\n",
    "        \n",
    "        hidden_size = 2\n",
    "        num_layers = 2\n",
    "        \n",
    "        cell_fw = None\n",
    "        cell_bw = None\n",
    "        if(rnn_type == 'rnn'):\n",
    "            cell_fw = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "            cell_bw = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "        elif(rnn_type == 'lstm'):\n",
    "            cell_fw = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "            cell_bw = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "            #state_is_tuple = True로 하면 Cell State와 Hidden State의 값을 tuple형태로 분리해서 보여줍니다.\n",
    "            \n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x_input, \n",
    "                                                          sequence_length=sequence_length,\n",
    "                                                          dtype=tf.float32)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"outputs of Multi_RNN\")\n",
    "        print(sess.run(outputs))\n",
    "        print(\"states of Multi_RNN\")\n",
    "        pprint.pprint(sess.run(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of Multi_RNN\n",
      "(array([[[-0.06758793, -0.01852215],\n",
      "        [-0.16376032, -0.03272057],\n",
      "        [-0.09567963, -0.06403921],\n",
      "        [-0.11466568, -0.09637608],\n",
      "        [ 0.06649007,  0.03735987],\n",
      "        [-0.05943045, -0.01769379],\n",
      "        [ 0.00136965, -0.04845091],\n",
      "        [ 0.01521786, -0.15079093]]], dtype=float32), array([[[ 0.1013485 , -0.06081938],\n",
      "        [ 0.03055528, -0.02659458],\n",
      "        [ 0.02441726,  0.02000265],\n",
      "        [-0.01413904,  0.01390083],\n",
      "        [-0.11112228, -0.04664689],\n",
      "        [-0.02333218, -0.14173329],\n",
      "        [ 0.03943175, -0.04680184],\n",
      "        [-0.00178974, -0.10911496]]], dtype=float32))\n",
      "states of Multi_RNN\n",
      "(LSTMStateTuple(c=array([[ 0.02939724, -0.25054047]], dtype=float32), h=array([[ 0.01521786, -0.15079093]], dtype=float32)),\n",
      " LSTMStateTuple(c=array([[ 0.25206664, -0.1366279 ]], dtype=float32), h=array([[ 0.1013485 , -0.06081938]], dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "Bi_RNN(x_input, sequence_length, rnn_type='lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output값과 State값이 데이터를 원래의 방향대로 넣어서(forward) 나온 값과 순서를 반대로 해서 넣은값(backward)값 2개로 나뉘어서 나오게 됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Bidirectional RNN\n",
    "Bidirectional RNN을 여러층 쌓아 Multi Layer Bidirectional RNN을 만들어 보도록 하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Multi_Bi_RNN(x_input, sequence_length, rnn_type='rnn'):\n",
    "    with tf.variable_scope('multi_bidirectional_rnn') as scope:\n",
    "        \n",
    "        hidden_size = 2\n",
    "        num_layers = 2\n",
    "        \n",
    "        cell_fw = None\n",
    "        cell_bw = None\n",
    "        if(rnn_type == 'rnn'):\n",
    "            cell_fw = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "            cell_bw = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "            \n",
    "        elif(rnn_type == 'lstm'):\n",
    "            cell_fw = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "            cell_bw = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)            \n",
    "            #state_is_tuple = True로 하면 Cell State와 Hidden State의 값을 tuple형태로 분리해서 보여줍니다.\n",
    "        cell_fw = tf.contrib.rnn.MultiRNNCell(cells=[cell_fw] * num_layers, state_is_tuple=True)\n",
    "        cell_bw = tf.contrib.rnn.MultiRNNCell(cells=[cell_bw] * num_layers, state_is_tuple=True)\n",
    "            \n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x_input, \n",
    "                                                          sequence_length=sequence_length,\n",
    "                                                          dtype=tf.float32)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"outputs of Bidirectional_Multi_RNN\")\n",
    "        print(sess.run(outputs))\n",
    "        print(\"states of Bidirectional_Multi_RNN\")\n",
    "        pprint.pprint(sess.run(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of Bidirectional_Multi_RNN\n",
      "(array([[[ 0.00756073,  0.00132258],\n",
      "        [ 0.00424517, -0.02354742],\n",
      "        [ 0.00700316, -0.04192907],\n",
      "        [ 0.00163326, -0.06305805],\n",
      "        [-0.0085106 , -0.07864803],\n",
      "        [ 0.00569872, -0.06843027],\n",
      "        [ 0.01326775, -0.07049034],\n",
      "        [ 0.02331219, -0.05905479]]], dtype=float32), array([[[ 0.02222335, -0.0222692 ],\n",
      "        [ 0.02394018, -0.02534211],\n",
      "        [ 0.02030206, -0.02387164],\n",
      "        [ 0.01505285, -0.01657416],\n",
      "        [ 0.02100322, -0.02833175],\n",
      "        [ 0.00979508, -0.01504568],\n",
      "        [-0.00529743,  0.00631799],\n",
      "        [-0.01081359,  0.01404889]]], dtype=float32))\n",
      "states of Bidirectional_Multi_RNN\n",
      "((LSTMStateTuple(c=array([[-0.24390453, -0.02500473]], dtype=float32), h=array([[-0.14028606, -0.01504857]], dtype=float32)),\n",
      "  LSTMStateTuple(c=array([[ 0.04581917, -0.11993713]], dtype=float32), h=array([[ 0.02331219, -0.05905479]], dtype=float32))),\n",
      " (LSTMStateTuple(c=array([[ 0.23882382,  0.0188105 ]], dtype=float32), h=array([[ 0.08789509,  0.00949991]], dtype=float32)),\n",
      "  LSTMStateTuple(c=array([[ 0.0453291 , -0.04480963]], dtype=float32), h=array([[ 0.02222335, -0.0222692 ]], dtype=float32))))\n"
     ]
    }
   ],
   "source": [
    "Multi_Bi_RNN(x_input, sequence_length, rnn_type='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
