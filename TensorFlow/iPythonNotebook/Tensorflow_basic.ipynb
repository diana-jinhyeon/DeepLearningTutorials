{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow란?\n",
    "Tensorflow란 딥러닝을 위해 구글 브레인(Google Brain)팀에서 만든 오픈소스 라이브러리이며 2015년 11월 9일 아파치 2.0 오픈소스 라이센스로 공개되었습니다. 본 문서는 1.1 version기준으로 작성되었습니다.\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://github.com/tensorflow/tensorflow\"><img src=\"https://www.tensorflow.org/images/tf_logo_transp.png\"></a><br><br>\n",
    "</div>\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow 설치\n",
    "본 장에서는 Tensorflow를 설치하는 것에 대해서 다룹니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Tensorflow 공식 홈페이지에 접속 후 OS에 맞는 버전 안내로 이동합니다. <br>\n",
    "본 문서에서는 Windows를 기준으로 설명하겠습니다.<br>\n",
    "https://www.tensorflow.org/install/\n",
    "<br>\n",
    "\n",
    "<img src=\"./assets/tf_1_install.png\"></img>\n",
    "<br>\n",
    "\n",
    "2.Anaconda로 설치하기 <br>\n",
    "먼저, Anaconda 의 공식 홈페이지에서 Python 3.6버전의 Anaconda 패키지를 설치합니다.<br>\n",
    "https://www.continuum.io/downloads#windows\n",
    "<br>\n",
    "\n",
    "<img src=\"./assets/tf_2_install.png\"></img>\n",
    "<img src=\"./assets/anaconda_install_py.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.conda environment 생성하기\n",
    "\n",
    "<blockquote><p>C:> conda create -n tensorflow </p></blockquote> <br>\n",
    "\n",
    "4.conda environment 활성화하기\n",
    "\n",
    "<blockquote><p>C:> activate tensorflow </p><br>\n",
    "(tensorflow)C:>  # Your prompt should change \n",
    "</blockquote> <br>\n",
    "\n",
    "5.tensorflow를 설치합니다<br>\n",
    "CPU 버전과 GPU버전 중에 맞는 버전을 설치하시면 됩니다.\n",
    "<br><br>\n",
    "5-1. CPU Version <br>\n",
    "<blockquote><p>(tensorflow)C:> pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl </p></blockquote> \n",
    "\n",
    "5-2. GPU Version <br>\n",
    "\n",
    "<blockquote><p>(tensorflow)C:> pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl</p></blockquote> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이제 아래의 코드가 실행이 된다면, 설치가 완료된 것입니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Hello, TensorFlow!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "sess.run(hello) # 'Hello, TensorFlow!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow의 주요 개념 배우기\n",
    "본 장에서는 Tensorflow를 이루는 주요 개념에 대해서 다뤄보도록 하겠습니다. <br>\n",
    "내용은 크게 다음과 같습니다.\n",
    "1. Tensor\n",
    "2. Computational graph\n",
    "3. Session (Run)\n",
    "4. Constants \n",
    "5. Variables\n",
    "6. Placeholder\n",
    "7. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "Tensor란 무엇일까요? <br>\n",
    "머신러닝, 딥러닝에서는 보통 복잡한 형태의 데이터를 다루게 됩니다. <br>\n",
    "이러한 데이터를 처리하기 위해서는 일반화된 형태(N 차원)가 필요합니다.<br>\n",
    "그것이 바로 Tensor라고 할 수 있습니다.<br><br>\n",
    "**Tensor**란 한마디로 정의하면 **N차원 array**입니다. <br><br>\n",
    "예를 들어 보겠습니다. <br>\n",
    "Vector는 1-d array이면서 동시에 1st-order tensor입니다. <br>\n",
    "Matrix는 2-d array이면서 동시에 2nd-order tensor입니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph 와 Session\n",
    "매우 중요한 개념이니 집중해서 보시기 바랍니다. <br>\n",
    "Tensorflow는 기본적으로 아래와 같이 나뉘어져 있습니다.<br>\n",
    "1. Computational Graph를 정의(Build)하는 영역 <br>\n",
    "2. Computational Graph를 실행(Run)하는 영역 (Session 안에서) <br>\n",
    "\n",
    "그렇기 때문에 코드를 볼때도 늘 이 두 관점으로 나눠서 보고 프로그래밍해야합니다.\n",
    "\n",
    "### 2. Computational Graph\n",
    "Computational graph란? 결국 graph의 형태로 연산을 정의하는 것을 의미합니다. <br>\n",
    "여기서 노드는 정의한 연산을, 엣지는 tensor를 의미합니다.<br>\n",
    "전체적으로 볼때, 엣지에 있는 tensor가 computational graph를 flow하면서 연산(Operation)이 일어난다고 보시면 됩니다.<br>\n",
    "아래의 1번과 2번 graph는 **같은** 그래프 입니다.<br><br>\n",
    "**2-1. Computational Graph (Name Scope X)** <br>\n",
    "아래는 Neural Network의 computational graph입니다. <br>\n",
    "Graph를 통해 정의된 연산을 살펴보면 Matrix의 multiplication 연산, cross entropy 계산과정이 아래에 나와있는 것을 확인 할 수 있습니다.<br>\n",
    "<img src=\"./assets/tf_3_cpgraph.png\"></img>\n",
    "<br><br>\n",
    "**2-2. Computational Graph (Name Scope O)** <br>\n",
    "Tensorflow에는 코드의 일정 영역에 대해 with문을 통해 Name scope 기능을 제공합니다. 이 기능은 추후에 자세히 다루도록 하겠습니다. 위에 있는 Computational graph에 Name scope 지정하면 아래와 같이 표현할 수 있습니다.\n",
    "<img src=\"./assets/tf_4_cpgraph.png\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Session\n",
    "Session이란? 위에서 정의한 Computational graph의 연산을 **실행(Run)**하는 부분이라고 생각하시면 됩니다. 세션은 Computational graph(Tensorflow Operations)를 인자(argument)로 받아서 실행해주는 일종의 Runner입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(30)\n",
    "\n",
    "# with문을 사용해서 세션 선언\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(sess.run(a)) #세션 안에 넣어서 실행하는 방법\n",
    "    \n",
    "    print(a.eval()) # 세션을 편하게 실행하기 위한 또 다른 방법\n",
    "\n",
    "# with문 없이 세션 선언\n",
    "sess = tf.Session()\n",
    "print(sess.run(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 Interactive session\n",
    "Tensorflow에서는 session외에 추가로 interactive session이라는 것을 제공합니다.<br>\n",
    "iPython과 같이 interactive shell 환경에서 매우 편리하게 사용 할 수 있습니다.<br>\n",
    "예를 들면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "a = tf.constant(30)\n",
    "\n",
    "# 보통 session 방식\n",
    "print(sess.run(a))\n",
    "\n",
    "# Interactive session 방식 \n",
    "print(a.eval()) # Operation을 session object안에 넣지 않고 간단하게 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습 환경에 iPython 환경이므로, 앞으로는 이 Interactive session을 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4,5 Constants & Variables\n",
    "상수와 변수 정의는 어떻게 하나요? <br>\n",
    "Tensorflow에서는 tf.constant, tf.Variable 형태로 상수와 변수를 선언할 수 있습니다. <br>\n",
    "### 4. Constant \n",
    "상수의 경우 tf.constant 를 통해 정의가 가능합니다. <br>\n",
    "선언할땐 dtype 과 name을 지정해주는 것이 좋습니다. <br>\n",
    "dtype의 경우에는 나중에 연산할 때 서로 타입이 맞아야하기 때문이며, 보통 index타입 같이 int형을 사용하는 경우를 제외하고 연산 속도 때문에 tf.float32 타입을 많이 사용합니다.<br>\n",
    "name은 변수에 대해서 디버깅 할때 혹은 나중에 tensorboard를 사용할때 도움이 되므로 중요한 변수는 지정해두는 것이 좋습니다.<br>\n",
    "constant는 다음과 같은 인자를 가질 수 있습니다. <br>\n",
    "- tf.constant(value, dtype=None, shape=None, name='Const')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input1 의 정보:  Tensor(\"input1:0\", shape=(4,), dtype=float32)\n",
      "[ 1.  1.  1.  1.]\n",
      "Input1 Shape:  (4,)\n",
      "Input1 Shape as list:  [4]\n",
      "\n",
      "Input2 의 정보:  Tensor(\"input2:0\", shape=(2, 4), dtype=float32)\n",
      "[[ 2.  2.  2.  2.]\n",
      " [ 2.  2.  2.  2.]]\n",
      "Input2 Shape:  (2, 4)\n",
      "Input2 Shape as list:  [2, 4]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "# input1 = tf.constant([1, 1, 1, 1]) 으로 간단하게 선언할 수도 있습니다.\n",
    "input1 = tf.constant([1, 1, 1, 1], dtype=tf.float32, name='input1')\n",
    "input2 = tf.constant([[2, 2, 2, 2],[2, 2, 2, 2]], dtype=tf.float32, name='input2')\n",
    "\n",
    "print(\"Input1 의 정보: \", input1)\n",
    "print(input1.eval())\n",
    "print(\"Input1 Shape: \", input1.get_shape())\n",
    "print(\"Input1 Shape as list: \", input1.get_shape().as_list())\n",
    "print(\"\")\n",
    "print(\"Input2 의 정보: \", input2)\n",
    "print(input2.eval())\n",
    "print(\"Input2 Shape: \", input2.get_shape())\n",
    "print(\"Input2 Shape as list: \", input2.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Variables\n",
    "Tensorflow에서 제공하는 변수에 대해서 알아보겠습니다. <br>\n",
    "변수는 상수와 비슷하지만 크게 다른점이 3가지가 있습니다. <br>\n",
    "1. 값을 변경 할 수 있습니다.\n",
    "2.  초기화를 꼭 해야합니다.\n",
    "3. trainable 여부를 지정할 수 있습니다.\n",
    "\n",
    "Variable은 다음과 같은 인자를 가질 수 있습니다.\n",
    "- tf.Variable(initial_value=None, trainable=True, name=None, variable_def=None, dtype=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight 변수 정보:  Tensor(\"weight/read:0\", shape=(2, 2), dtype=float32)\n",
      "weight 변수 이름:  weight:0\n",
      "weight 변수 Shape:  (2, 2)\n",
      "변수 초기화 후 weight값\n",
      "[[-0.02479463  0.20258832]\n",
      " [-0.1016199   0.12955454]]\n",
      "[[-0.02479463  0.20258832]\n",
      " [-0.1016199   0.12955454]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 이 변수는 training할 때 업데이트가 가능합니다.\n",
    "weight = tf.Variable(tf.random_normal([2,2], stddev=0.1), name='weight')\n",
    "\n",
    "# 이 변수는 training할 때 업데이트 되지 않습니다.\n",
    "static_weight = tf.Variable(tf.random_normal([2,2], stddev=0.1), trainable=False, name='static_weight')\n",
    "\n",
    "print(\"weight 변수 정보: \", weight)\n",
    "print(\"weight 변수 이름: \", weight.name)\n",
    "print(\"weight 변수 Shape: \", weight.get_shape())\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer() # 초기화 Operation 정의\n",
    "sess.run(init) # 초기화 Operation 실행\n",
    "# tf.global_variables_initializer().run() # interactive session 스타일로 실행\n",
    "\n",
    "print(\"변수 초기화 후 weight값\")\n",
    "print(weight.eval()) # Interactive session 방식\n",
    "print(sess.run(weight)) # 기존 session 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.global_variables\n",
    "현재 선언된 변수들을 모두 담고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변수 명:  weight:0 , Shape:  (2, 2)\n",
      "변수 명:  static_weight:0 , Shape:  (2, 2)\n"
     ]
    }
   ],
   "source": [
    "# 현재 선언된 모든 변수를 출력합니다.\n",
    "for var in tf.global_variables ():\n",
    "    print(\"변수 명: \", var.name, \", Shape: \", var.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. Sharing variables\n",
    "지금까지 Variable에 대해서 배웠습니다. 지금부터는 Sharing Variable에 대해서 배워보려고 합니다. Sharing Variable은 왜 필요할까요? <br>\n",
    "1. 데이터나 혹은 문제 상황이 복잡할 경우 이에 따라 우리가 만드는 모델 또한 복잡해 질 수 있습니다. \n",
    "2. 예를 들어 두개의 이미지를 비교하는 Task를 가정해 봅시다. image1과 image2를 비교 하고 싶고 이때 우리는 같은 Parameter(weight)를 갖는 모델을 '재사용'할 것입니다.\n",
    "3. 다음과 같이 모델을 함수의 형태로 정의합니다. 본 예제에서는 Convolutional Neural Network를 사용했습니다. 이에 대해서는 추후 자세히 다룰 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_image_filter(input_images):\n",
    "    \n",
    "    # 첫번째 Convolutional layer와 Max pooling layer     \n",
    "    W_conv1 = tf.Variable(tf.random_normal([5, 5, 1, 32], stddev=0.1), name='W_conv1') # (가로길이,세로길이,채널 차원수,필터개수)    \n",
    "    b_conv1 = tf.Variable(tf.random_normal([32], stddev=0.1), name='b_conv1')\n",
    "    cnn_output = tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    h_conv1 = tf.nn.relu(cnn_output + b_conv1, name=\"h_conv1\")\n",
    "    h_pool1 =     tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME', name=\"h_pool1\")\n",
    "    \n",
    "    # 두번째 Convolutional layer와 Max pooling layer\n",
    "    W_conv2 = tf.Variable(tf.random_normal([5, 5, 32, 64], stddev=0.1), name='W_conv2') # (가로길이,세로길이,채널 차원수,필터개수)    \n",
    "    b_conv2 = tf.Variable(tf.random_normal([64], stddev=0.1), name='b_conv2')\n",
    "    cnn2_output = tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    h_conv2 = tf.nn.relu(cnn2_output + b_conv2, name=\"h_conv2\")\n",
    "    h_pool2 =     tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME', name=\"h_pool2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 함수를 정의하고 실제 이미지에 대해서 분석할때 다음과 같이 호출을 한다고 해보겠습니다. \n",
    "<blockquote>\n",
    "<p>result1 = my_image_filter(image1)</p>\n",
    "<p>result2 = my_image_filter(image2) </p>\n",
    "</blockquote>\n",
    "이렇게 호출을 하면 실제로는 변수를 재사용 할 수가 없고 **변수가 2배로 생성 되는 문제**가 발생합니다. <br>\n",
    "이러한 문제를 해결하고, 기존에 있는 변수와 복잡한 모델을 재사용하기 위해서는 좀 더 편리한 방법이 필요했습니다. 그리고 그것을 해결한 것이 Sharing variable이라는 개념입니다.<br>\n",
    "Sharing Variable은 다음 3가지의 요소로 구성됩니다. <br>\n",
    "- tf.variable_scope\n",
    "- tf.get_variable\n",
    "- scope.reuse_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.variable_scope\n",
    "프로그래밍을 하다보면 특정 그룹의 변수를 재사용 또는 그룹핑 해야할 때가 있습니다.<br>\n",
    "variable_scope는 Variable에 대한 Namespace를 할당함으로써 이러한 문제를 해결해줍니다.<br>\n",
    "Tensorflow에서 Variable scope는 보통 with 문과 함께 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant:0\n",
      "layer1/constant:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "c_without_scope = tf.constant([1,1], name=\"constant\")\n",
    "print(c_without_scope.name)\n",
    "\n",
    "with tf.variable_scope('layer1'):\n",
    "    c_with_scope = tf.constant([1,1], name=\"constant\")\n",
    "    print(c_with_scope.name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.get_variable\n",
    "tf.get_variable은 tf.Variable의 또 다른 형태로 보셔도 되지만 약간 차이가 납니다. 왜냐하면 tf.Variable과 같이 변수를 **생성**하기도 하지만 때론 **재사용**하기도 하기 때문입니다. <br>\n",
    "get_variable은 다음과 같은 인자를 가질 수 있습니다.<br>\n",
    "- tf.get_variable(name, shape=None, dtype=None, initializer=None, trainable=True)\n",
    "\n",
    "**Case 1**: Variable의 Scope가 재사용(reuse)으로 설정되어 있지 **않은** 경우 <br>\n",
    "- tf.get_variable_scope().reuse == False <br>\n",
    "\n",
    "이때는 tf.Variable 처럼 **새롭게 변수를 생성**합니다. **get_variable은 Variable과 달리** value를 직접 넣어서 초기화해주는 것이 아닌 **초기화를 해주는 initializer를 넣어줘서 초기화**를 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.11536598  0.76762462]\n",
      " [ 0.90300667 -0.71181428]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#initializer_for_variable = tf.random_normal_initializer(mean=0, stddev=0.1)\n",
    "initializer_for_variable = tf.contrib.layers.xavier_initializer() # Xavier Glorot and Yoshua Bengio (2010): Understanding the difficulty of training deep feedforward neural networks. \n",
    "v = tf.get_variable('v', shape=[2, 2], initializer=initializer_for_variable)\n",
    "tf.global_variables_initializer().run()\n",
    "print(v.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2**: Variable의 Scope가 재사용(resue)으로 설정되어 **있는** 경우 <br>\n",
    "- tf.get_variable_scope().reuse == True <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo/v:0\n",
      "foo/v:0\n",
      "[[ 0.73221183 -0.30515927]\n",
      " [-0.7708472   0.49378467]]\n",
      "[[ 0.73221183 -0.30515927]\n",
      " [-0.7708472   0.49378467]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "initializer_for_variable = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "with tf.variable_scope(\"foo\"): #재사용하려면 항상 선언은 되어있어야 하는 코드\n",
    "    v = tf.get_variable(\"v\", shape=[2, 2], initializer=initializer_for_variable)\n",
    "    print(v.name)\n",
    "\n",
    "with tf.variable_scope(\"foo\", reuse=True):\n",
    "    v1 = tf.get_variable(\"v\", shape=[2, 2], initializer=initializer_for_variable)\n",
    "    # v1 = tf.get_variable(\"v\")  # 기존에 변수가 존재하면 intializer없어도 됨\n",
    "    print(v1.name)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "    \n",
    "print(v.eval())\n",
    "print(v1.eval())\n",
    "assert v1 is v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 경우(reuse=True)에는 Variable의 name **\"v\"**에 대해서<br>\n",
    "Variable Scope Name + Variable Name 으로 변수를 찾기 시작합니다.<br>\n",
    "Ex) foo/v 경로로 변수가 있는지 탐색합니다.<br>\n",
    "변수가 있는지 확인하고 있으면 해당 변수를 사용하고 찾지 못할 경우 ValueError 에러가 발생하게 됩니다.<br>\n",
    "Ex) ValueError: Variable foo/v does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scope.reuse_variables\n",
    "scope에 대해서 reuse하는 방법은 두가지가 있습니다. 편한 방법을 선택하시면 됩니다.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo/v:0\n",
      "foo/v:0\n",
      "[[ 1.12565911  0.63347077]\n",
      " [ 0.21556544  0.24685729]]\n",
      "[[ 1.12565911  0.63347077]\n",
      " [ 0.21556544  0.24685729]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "initializer_for_variable = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# reuse 코드 (맨처음 선언한 코드는 위쪽 코드에 있습니다.)\n",
    "with tf.variable_scope(\"foo\", reuse=True):\n",
    "    v = tf.get_variable(\"v\", shape=[2, 2], initializer=initializer_for_variable)\n",
    "    print(v.name)\n",
    "with tf.variable_scope(\"foo\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    v1 = tf.get_variable(\"v\")\n",
    "    print(v1.name)\n",
    "\n",
    "\n",
    "tf.global_variables_initializer().run()    \n",
    "\n",
    "print(v.eval())\n",
    "print(v1.eval())\n",
    "\n",
    "assert v1 is v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Placeholder\n",
    "Placeholder가 뭘까요? Tensorflow에서는 이미 constant와 Variable을 제공하고 있습니다. Placeholder라는게 굳이 필요할까요?<br>\n",
    "우리가 데이터를 입력 시킬때 보통 데이터는 Python 객체나 Matrix의 경우 Numpy 객체의 형태를 갖고 있습니다. 이러한 데이터를 Tensorflow에서 사용해주기 위해서는 Tensorflow의 자료구조인 Tensor형태로 변형해주어야 합니다. \n",
    "- tf.convert_to_tensor(numpy객체)\n",
    "\n",
    "위의 함수는 이러한 역할을 충실히 수행해 줍니다. 그러나 tf.convert_to_tensor는 데이터를 한번 할당하면 끝이기 때문에 확장성이 없습니다. <br>\n",
    "이러한 배경에서 이것을 대신해서 나온 Input 값을 담는 특별한 변수를 Placeholder다 라고 생각하시면 됩니다.<br><br>\n",
    "즉, 정리하면 Placeholder는 **실제 Input data를 담는 자료구조** 라고 생각하시면 됩니다.<br><br>\n",
    "Placeholder는 특별한 특징을 가집니다. 매우 중요하니 집중해서 보셔야합니다. Tensorflow가 Computational Graph 정의(Build)영역과 Computational Graph 실행(Run)영역(session)으로 나뉜다는건 위에서 말씀드렸습니다. <br> \n",
    "이 말을 다른말로 하면 **Running 영역(session으로 실행한 영역)에는 실제 그 코드 라인에 값이 들어있지만 Computational Graph를 정의(Build)한 코드 라인에는 실제 값이 들어있는것은 아니다** 라고 할 수 있습니다. <br><br>\n",
    "\n",
    "placeholder의 선언과 feed 형태\n",
    "- 변수명 = tf.placeholder(dtype, shape=None, name=None) \n",
    "- ops.eval(feed_dict={변수명:데이터})\n",
    "\n",
    "예제를 통해 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"placeholder_x:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"placeholder_x\") # None은 정해지지 않았을때, 혹은 Batch로 처리할때 사용합니다.\n",
    "y = tf.placeholder(tf.float32, [None, 2], name=\"placeholder_y\")\n",
    "\n",
    "tf.global_variables_initializer().run()    \n",
    "\n",
    "print(x)\n",
    "# print(x.eval()) #실행하면 에러 발생합니다. 값이 없기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주석이 달려있는 print(x.eval())을 실행하면 에러가 납니다. 왜냐하면 Placeholder 자체는 값이 들어있지 않은데, 값을 넣어주지 않은 상태로 실행했기 때문입니다. 값은 어떻게 넣어줄 수 있을까요? <br>\n",
    "Placeholder는 feed_dict로 key, value 형태의 값을 넣어줄 수 있습니다. <br><br>\n",
    "\n",
    "**주의**<br>\n",
    "1. 이때 값은 전처리된 형태로 넣어줘야 합니다. 왜일까요? <br>\n",
    "Tensorflow내에서는 Tensor 객체로 연산이 이루어지기 때문에 일반 Python 객체를 다루듯이 처리하기가 어렵습니다. 또한 실제 session에서 실행되기 전까지 placeholder에는 빈 값이 들어가 있기 때문에 예를 들면, placeholder의 길이를 통한 for loop 돌리기 같은 방법은 사용 할 수 없습니다.\n",
    "2. 첫번째 차원은 Batch_size입니다. 이게 무슨 말일까요? <br>\n",
    "Tensorflow에서 Input data를 처리할때 첫번째 차원을 Batch_size로 지정했습니다. 이것은 매우 중요합니다. 앞으로 데이터를 처리할때 계속 이 부분을 염두해두어야합니다. 직관적으로 Input data를 Placeholder에 넣었을 때 첫번째 차원이 Matrix의 row일 것 같지만 Tensorflow에서는 Batch_size라는걸 기억해두세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"placeholder_x_1:0\", shape=(?, 2), dtype=float32)\n",
      "[[ 1.  2.]]\n",
      "[[ 1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"placeholder_x\") # None은 정해지지 않았을때, 혹은 Batch로 처리할때 사용합니다.\n",
    "y = tf.placeholder(tf.float32, [None, 2], name=\"placeholder_y\")\n",
    "\n",
    "# 아직 실제 값이 없는 부분 \n",
    "print(x)\n",
    "\n",
    "#실제 값이 들어가게 되는 부분\n",
    "# 첫번째 차원은 Batch size이기 때문에 [1,2]에서 괄호를 하나 더 씌워서 차원을 추가해줍니다.\n",
    "print(x.eval(feed_dict={x:[[1,2]]})) # InteractiveSession 방식\n",
    "print(sess.run(x, feed_dict={x:[[1,2]]})) # 기본 session 방식\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder는 앞으로 Input data를 다룰 때 마다 계속 나오기 때문에 꼭 기억하시는게 좋습니다. Placeholder 부분을 위에서 봤던 Computational Graph 그림으로 보면 아래와 같습니다. 빨간색으로 강조된 부분이 placeholder가 들어가는 부분입니다.\n",
    "<img src=\"./assets/tf_5_cpgraph.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Functions\n",
    "본 장에서는 Tensor를 다루기 위한 기본적인 함수들을 다룰 것입니다. 종종 코드에 등장하기 때문에 익혀두시면 도움이 많이 될 것입니다. <br>\n",
    "- tf.random_normal\n",
    "- tf.argmax\n",
    "- tf.reduce_sum\n",
    "- tf.equal\n",
    "- tf.expand_dims\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.random_normal\n",
    "나중에 변수를 초기화할때 주로 많이 쓰입니다.<br>\n",
    "Normal distribution으로부터 랜덤하게 값을 얻습니다. <br>\n",
    "tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"random_normal_2:0\", shape=(5,), dtype=float32)\n",
      "[-0.02228741  0.05121428 -0.05145186 -0.03313956  0.02715646]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "random_value = tf.random_normal([5], stddev=0.1)\n",
    "print(random_value)\n",
    "print(random_value.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.argmax\n",
    "나중에 output벡터에서 어떤 클래스로 분류했는지 알아낼때 주로 많이 쓰입니다.<br>\n",
    "Input Tensor에서 가장 큰 값을 갖는 element의 argument(index)를 반환합니다. <br>\n",
    "반환되는 argument(Index)는 0번부터 시작합니다.<br>\n",
    "tf.argmax(input, axis=None, name=None, dimension=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a: \n",
      "[1 2 3]\n",
      "argmax a\n",
      "2\n",
      "\n",
      "Tensor b: \n",
      "[[2 4 1]\n",
      " [7 2 3]]\n",
      "argmax b\n",
      "[1 0 1]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "a = tf.constant([1,2,3])\n",
    "b = tf.constant([[2,4,1],[7,2,3]])\n",
    "\n",
    "print(\"Tensor a: \")\n",
    "print(a.eval())\n",
    "print(\"argmax a\")\n",
    "print(tf.argmax(a, axis=0).eval())\n",
    "print(\"\")\n",
    "print(\"Tensor b: \")\n",
    "print(b.eval())\n",
    "print(\"argmax b\")\n",
    "print(tf.argmax(b, axis=0).eval()) #row단위(Batch), [2 7]중에 큰 것, [4 2]중에 큰 것, [1 3]중에 큰 것의 index 반환\n",
    "print(tf.argmax(b, axis=1).eval()) # [2 4 1]중에 큰 것과 [7 2 3]중에 큰 것의 index반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.reduce_sum\n",
    "나중에 Tensor의 각 loss를 합치는 계산할때 주로 많이 쓰입니다. <br>\n",
    "\n",
    "tf.reduce_sum(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "[[1 1 1]\n",
      " [1 1 1]]\n",
      "\n",
      "tf.reduce_sum 결과들:\n",
      "전체 matrix에 대한 reduce_sum\n",
      "6\n",
      "0번 aixs에 대한 reduce_sum\n",
      "[2 2 2]\n",
      "1번 aixs에 대한 reduce_sum\n",
      "[3 3]\n",
      "0번 aixs에 대한 reduce_sum + Dimension 유지\n",
      "[[3]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.constant([[1, 1, 1],[1, 1, 1]])\n",
    "print(\"x\")\n",
    "print(x.eval())\n",
    "\n",
    "print(\"\")\n",
    "print(\"tf.reduce_sum 결과들:\")\n",
    "print(\"전체 matrix에 대한 reduce_sum\")\n",
    "print(tf.reduce_sum(x).eval()) # ==> 6\n",
    "print(\"0번 aixs에 대한 reduce_sum\")\n",
    "print(tf.reduce_sum(x, 0).eval()) # ==> [2, 2, 2]\n",
    "print(\"1번 aixs에 대한 reduce_sum\")\n",
    "print(tf.reduce_sum(x, 1).eval()) # ==> [3, 3]\n",
    "print(\"0번 aixs에 대한 reduce_sum + Dimension 유지\")\n",
    "print(tf.reduce_sum(x, 1, keep_dims=True).eval()) # ==> [[3], [3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.equal\n",
    "나중에 tf.argmax통해 Prediction값과 Label값의 index를 비교해서 Accuracy를 계산할 때 주로 많이 쓰입니다. <br>\n",
    "x,y를 비교 한다고 가정했을 때, (x == y) 에 대해서 element-wise로 비교해서 True, False 값을 리턴해줍니다. <br>\n",
    "tf.equal(x, y, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "[[0 1 0]\n",
      " [1 1 1]]\n",
      "b\n",
      "[[1 1 1]\n",
      " [1 1 1]]\n",
      "a와 b 비교\n",
      "[[False  True False]\n",
      " [ True  True  True]]\n",
      "a와 1 비교\n",
      "[[False  True False]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "a = tf.constant([[0, 1, 0], [1, 1, 1]])\n",
    "b = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
    "\n",
    "print(\"a\")\n",
    "print(a.eval())\n",
    "print(\"b\")\n",
    "print(b.eval())\n",
    "\n",
    "print(\"a와 b 비교\")\n",
    "print(tf.equal(a, b).eval())\n",
    "\n",
    "# tf.equal은 broadcasting을 지원합니다.(차원이 안맞아도 맞춰주는 기능)\n",
    "\n",
    "print(\"a와 1 비교\")\n",
    "print(tf.equal(a, 1).eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.expand_dims\n",
    "Tensorflow의 Input의 첫번째 차원은 Batch_size 입니다. tf.expand_dims은 나중에 CNN에서 2d 이미지(width,height)를 batch 2d 이미지(batch_size,width,height)로 변환할때 많이 쓰입니다.<br>\n",
    "\n",
    "tf.expand_dims(input, axis=None, name=None, dim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_9:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"Const_10:0\", shape=(2, 3, 5), dtype=int32)\n",
      "\n",
      "tf.expand_dims 적용 후\n",
      "t  0번 axis expands:  Tensor(\"ExpandDims:0\", shape=(1, 2), dtype=int32)\n",
      "t  1번 axis expands:  Tensor(\"ExpandDims_1:0\", shape=(2, 1), dtype=int32)\n",
      "t -1번 axis expands:  Tensor(\"ExpandDims_2:0\", shape=(2, 1), dtype=int32)\n",
      "t2 0번 axis expands:  Tensor(\"ExpandDims_3:0\", shape=(1, 2, 3, 5), dtype=int32)\n",
      "t2 2번 axis expands:  Tensor(\"ExpandDims_4:0\", shape=(2, 3, 1, 5), dtype=int32)\n",
      "t2 3번 axis expands:  Tensor(\"ExpandDims_5:0\", shape=(2, 3, 5, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "t = tf.constant([0, 1])\n",
    "t2 = tf.constant([[[0, 1, 2, 3, 4],[0, 1, 2, 3, 4],[0, 1, 2, 3, 4]],\n",
    "                 [[0, 1, 2, 3, 4],[0, 1, 2, 3, 4],[0, 1, 2, 3, 4]]])\n",
    "\n",
    "print(t)\n",
    "print(t2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"tf.expand_dims 적용 후\")\n",
    "\n",
    "# 't' is a tensor of shape [2]\n",
    "print(\"t  0번 axis expands: \", tf.expand_dims(t, 0)) # ==> [1, 2]\n",
    "print(\"t  1번 axis expands: \", tf.expand_dims(t, 1)) # ==> [2, 1]\n",
    "print(\"t -1번 axis expands: \", tf.expand_dims(t, -1)) # ==> [2, 1]\n",
    "\n",
    "# 't2' is a tensor of shape [2, 3, 5]\n",
    "print(\"t2 0번 axis expands: \", tf.expand_dims(t2, 0)) # ==> [1, 2, 3, 5]\n",
    "print(\"t2 2번 axis expands: \", tf.expand_dims(t2, 2)) # ==> [2, 3, 1, 5]\n",
    "print(\"t2 3번 axis expands: \", tf.expand_dims(t2, 3)) # ==> [2, 3, 5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
